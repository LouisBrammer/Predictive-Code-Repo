{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification for positive negative.\n",
    "\n",
    "### Input: IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMDB DATASET\n",
    "\n",
    "# Function to load reviews from a directory\n",
    "def load_reviews(directory, label):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "                reviews.append(f.read())\n",
    "                labels.append(label)\n",
    "    return reviews, labels\n",
    "\n",
    "#set directories wihtin GitHub!\n",
    "train_dir = \"data/aclImdb/train\"\n",
    "test_dir = \"data/aclImdb/test\"\n",
    "train_pos_reviews, train_pos_labels = load_reviews(os.path.join(train_dir, 'pos'), 1)\n",
    "train_neg_reviews, train_neg_labels = load_reviews(os.path.join(train_dir, 'neg'), 0)\n",
    "\n",
    "# Load test data\n",
    "test_pos_reviews, test_pos_labels = load_reviews(os.path.join(test_dir, 'pos'), 1)\n",
    "test_neg_reviews, test_neg_labels = load_reviews(os.path.join(test_dir, 'neg'), 0)\n",
    "\n",
    "# Combine positive and negative reviews\n",
    "train_reviews = train_pos_reviews + train_neg_reviews\n",
    "train_labels = train_pos_labels + train_neg_labels\n",
    "test_reviews = test_pos_reviews + test_neg_reviews\n",
    "test_labels = test_pos_labels + test_neg_labels\n",
    "\n",
    "# Shuffle the data\n",
    "train_data = list(zip(train_reviews, train_labels))\n",
    "test_data = list(zip(test_reviews, test_labels))\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)\n",
    "\n",
    "\n",
    "train_reviews, train_labels = zip(*train_data)\n",
    "test_reviews, test_labels = zip(*test_data)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_reviews = np.array(train_reviews)\n",
    "train_labels = np.array(train_labels)\n",
    "test_reviews = np.array(test_reviews)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100005 files belonging to 2 classes.\n",
      "Found 100005 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Miguels approach for reading the data\n",
    "\n",
    "# Base path for the dataset\n",
    "dataset_path = 'data/aclImdb/'\n",
    "\n",
    "train_dataset = keras.utils.text_dataset_from_directory(os.path.expanduser(dataset_path), batch_size=32)\n",
    "valid_dataset = keras.utils.text_dataset_from_directory(os.path.expanduser(dataset_path), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Load the pretrained embeddings\n",
    "path_to_glove_file = \"data/glove.6B/glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(os.path.expanduser(path_to_glove_file)) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 13:05:06.204865: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "tokenizer = keras.layers.TextVectorization(max_tokens=max_tokens, output_sequence_length=max_length, output_mode=\"int\")\n",
    "\n",
    "train_dataset_text_only = train_dataset.map(lambda x, y: x)\n",
    "\n",
    "tokenizer.adapt(train_dataset_text_only)\n",
    "\n",
    "train_dataset_int = train_dataset.map(lambda x, y: (tokenizer(x), y), num_parallel_calls=4)\n",
    "\n",
    "valid_dataset_int = valid_dataset.map(lambda x, y: (tokenizer(x), y), num_parallel_calls=4)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "vocabulary = tokenizer.get_vocabulary()\n",
    "\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">34,048</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │  \u001b[38;5;34m2,000,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m34,048\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,034,113</span> (7.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,034,113\u001b[0m (7.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,113</span> (133.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,113\u001b[0m (133.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> (7.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,000,000\u001b[0m (7.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 153ms/step - accuracy: 0.7489 - loss: 0.5709 - val_accuracy: 0.7500 - val_loss: 0.5622\n",
      "Epoch 2/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1089s\u001b[0m 348ms/step - accuracy: 0.7512 - loss: 0.5633 - val_accuracy: 0.7500 - val_loss: 0.5610\n",
      "Epoch 3/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2351s\u001b[0m 752ms/step - accuracy: 0.7512 - loss: 0.5626 - val_accuracy: 0.7500 - val_loss: 0.5603\n",
      "Epoch 4/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 156ms/step - accuracy: 0.7514 - loss: 0.5615 - val_accuracy: 0.7500 - val_loss: 0.5600\n",
      "Epoch 5/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 155ms/step - accuracy: 0.7513 - loss: 0.5610 - val_accuracy: 0.7500 - val_loss: 0.5591\n",
      "Epoch 6/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 155ms/step - accuracy: 0.7512 - loss: 0.5613 - val_accuracy: 0.7500 - val_loss: 0.5581\n",
      "Epoch 7/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 155ms/step - accuracy: 0.7514 - loss: 0.5602 - val_accuracy: 0.7500 - val_loss: 0.5574\n",
      "Epoch 8/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 155ms/step - accuracy: 0.7513 - loss: 0.5593 - val_accuracy: 0.7501 - val_loss: 0.5561\n",
      "Epoch 9/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 160ms/step - accuracy: 0.7514 - loss: 0.5590 - val_accuracy: 0.7501 - val_loss: 0.5552\n",
      "Epoch 10/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 156ms/step - accuracy: 0.7515 - loss: 0.5576 - val_accuracy: 0.7501 - val_loss: 0.5536\n",
      "Epoch 11/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 157ms/step - accuracy: 0.7518 - loss: 0.5574 - val_accuracy: 0.7501 - val_loss: 0.5519\n",
      "Epoch 12/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 157ms/step - accuracy: 0.7519 - loss: 0.5556 - val_accuracy: 0.7502 - val_loss: 0.5493\n",
      "Epoch 13/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 157ms/step - accuracy: 0.7516 - loss: 0.5550 - val_accuracy: 0.7508 - val_loss: 0.5465\n",
      "Epoch 14/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 161ms/step - accuracy: 0.7519 - loss: 0.5527 - val_accuracy: 0.7507 - val_loss: 0.5444\n",
      "Epoch 15/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 160ms/step - accuracy: 0.7518 - loss: 0.5507 - val_accuracy: 0.7511 - val_loss: 0.5433\n",
      "Epoch 16/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 160ms/step - accuracy: 0.7524 - loss: 0.5493 - val_accuracy: 0.7515 - val_loss: 0.5386\n",
      "Epoch 17/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 158ms/step - accuracy: 0.7533 - loss: 0.5474 - val_accuracy: 0.7522 - val_loss: 0.5354\n",
      "Epoch 18/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 157ms/step - accuracy: 0.7532 - loss: 0.5453 - val_accuracy: 0.7541 - val_loss: 0.5322\n",
      "Epoch 19/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 160ms/step - accuracy: 0.7538 - loss: 0.5423 - val_accuracy: 0.7544 - val_loss: 0.5317\n",
      "Epoch 20/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 158ms/step - accuracy: 0.7541 - loss: 0.5416 - val_accuracy: 0.7551 - val_loss: 0.5274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1758519a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "embedded = embedding_layer(inputs)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(32))(embedded)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary() \n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    "\n",
    "model.fit(train_dataset_int, validation_data=valid_dataset_int, epochs=20, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"LSTM_IMDB_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'conv1d' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'conv1d_1' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'conv1d_2' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 125ms/step - accuracy: 0.7326 - loss: 0.6071 - val_accuracy: 0.7500 - val_loss: 0.5557\n",
      "Epoch 2/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 69ms/step - accuracy: 0.7513 - loss: 0.5624 - val_accuracy: 0.7504 - val_loss: 0.5504\n",
      "Epoch 3/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 73ms/step - accuracy: 0.7511 - loss: 0.5572 - val_accuracy: 0.7519 - val_loss: 0.5318\n",
      "Epoch 4/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 70ms/step - accuracy: 0.7522 - loss: 0.5482 - val_accuracy: 0.7608 - val_loss: 0.5144\n",
      "Epoch 5/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 72ms/step - accuracy: 0.7555 - loss: 0.5353 - val_accuracy: 0.7671 - val_loss: 0.4912\n",
      "Epoch 6/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 77ms/step - accuracy: 0.7603 - loss: 0.5233 - val_accuracy: 0.7814 - val_loss: 0.4695\n",
      "Epoch 7/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 75ms/step - accuracy: 0.7658 - loss: 0.5080 - val_accuracy: 0.7963 - val_loss: 0.4461\n",
      "Epoch 8/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 77ms/step - accuracy: 0.7720 - loss: 0.4965 - val_accuracy: 0.7983 - val_loss: 0.4261\n",
      "Epoch 9/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 78ms/step - accuracy: 0.7797 - loss: 0.4825 - val_accuracy: 0.8182 - val_loss: 0.4059\n",
      "Epoch 10/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 75ms/step - accuracy: 0.7858 - loss: 0.4676 - val_accuracy: 0.8235 - val_loss: 0.3918\n",
      "Epoch 11/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 74ms/step - accuracy: 0.7910 - loss: 0.4573 - val_accuracy: 0.8357 - val_loss: 0.3760\n",
      "Epoch 12/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 139ms/step - accuracy: 0.7957 - loss: 0.4478 - val_accuracy: 0.8421 - val_loss: 0.3599\n",
      "Epoch 13/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 72ms/step - accuracy: 0.7987 - loss: 0.4428 - val_accuracy: 0.8394 - val_loss: 0.3539\n",
      "Epoch 14/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 73ms/step - accuracy: 0.8056 - loss: 0.4285 - val_accuracy: 0.8477 - val_loss: 0.3421\n",
      "Epoch 15/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 73ms/step - accuracy: 0.8088 - loss: 0.4226 - val_accuracy: 0.8593 - val_loss: 0.3270\n",
      "Epoch 16/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 73ms/step - accuracy: 0.8137 - loss: 0.4155 - val_accuracy: 0.8659 - val_loss: 0.3181\n",
      "Epoch 17/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 76ms/step - accuracy: 0.8170 - loss: 0.4082 - val_accuracy: 0.8462 - val_loss: 0.3213\n",
      "Epoch 18/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 79ms/step - accuracy: 0.8200 - loss: 0.3990 - val_accuracy: 0.8773 - val_loss: 0.2980\n",
      "Epoch 19/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 86ms/step - accuracy: 0.8225 - loss: 0.3959 - val_accuracy: 0.8636 - val_loss: 0.2983\n",
      "Epoch 20/20\n",
      "\u001b[1m3126/3126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 89ms/step - accuracy: 0.8269 - loss: 0.3895 - val_accuracy: 0.8820 - val_loss: 0.2844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x175850710>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN‑based sentiment classifier (assumes `embedding_layer`,\n",
    "# `train_dataset_int`, and `valid_dataset_int` already exist)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ── Model definition ──────────────────────────────────────────────\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "# parallel n‑gram feature extractors\n",
    "c3 = layers.Conv1D(128, 3, activation=\"relu\")(x)\n",
    "c4 = layers.Conv1D(128, 4, activation=\"relu\")(x)\n",
    "c5 = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "\n",
    "# global max‑pool each feature map\n",
    "p3 = layers.GlobalMaxPooling1D()(c3)\n",
    "p4 = layers.GlobalMaxPooling1D()(c4)\n",
    "p5 = layers.GlobalMaxPooling1D()(c5)\n",
    "\n",
    "x = layers.concatenate([p3, p4, p5])\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# ── Training ──────────────────────────────────────────────────────\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset_int,\n",
    "    validation_data=valid_dataset_int,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CNN_IMDB_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
