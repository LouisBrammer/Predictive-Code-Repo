{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04659649",
   "metadata": {},
   "source": [
    "Let's in this case instead of using GloVe let's use having the same structure as before the BERT tranformer, using its embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c10cce",
   "metadata": {},
   "source": [
    "As a note the previous model in the goemotions_training_template had an accuracy during training of 0.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48292074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (207814, 37)\n",
      "\n",
      "First few rows of the dataset:\n",
      "                                                text       id  \\\n",
      "0                                    That game hurt.  eew5j0j   \n",
      "1     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
      "2                                 Man I love reddit.  eeibobj   \n",
      "3  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
      "4  Right? Considering it’s such an important docu...  eespn2i   \n",
      "\n",
      "                author            subreddit    link_id   parent_id  \\\n",
      "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
      "1             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
      "2        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
      "3  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
      "4         ImperialBoss           TrueReddit  t3_aizyuz  t1_eesoak0   \n",
      "\n",
      "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
      "0  1.548381e+09         1                 False           0  ...     0   \n",
      "1  1.546428e+09        37                 False           0  ...     0   \n",
      "2  1.547965e+09        18                 False           0  ...     1   \n",
      "3  1.546669e+09         2                 False           0  ...     0   \n",
      "4  1.548280e+09        61                 False           0  ...     0   \n",
      "\n",
      "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0            0         0      0            0       0        0        1   \n",
      "1            0         0      0            0       0        0        0   \n",
      "2            0         0      0            0       0        0        0   \n",
      "3            0         0      0            0       0        0        0   \n",
      "4            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        1  \n",
      "2         0        0  \n",
      "3         0        1  \n",
      "4         0        0  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "\n",
      "Column names:\n",
      "['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "\n",
      "Number of examples: 207814\n",
      "Example text: That game hurt....\n",
      "Example label shape: (27,)\n",
      "\n",
      "Checking for and removing rows with single-instance emotion labels...\n",
      "No single-instance labels found. Proceeding with original data.\n",
      "\n",
      "Splitting data into train, validation, and test sets...\n",
      "\n",
      "Train set size: 166251\n",
      "Validation set size: 20781\n",
      "Test set size: 20782\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split\n",
    "\n",
    "# Hides the GPU from TensorFlow if needed, adjust based on your setup\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "# If you want to use GPU, ensure TensorFlow is installed with GPU support\n",
    "# and remove or comment the line above.\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "# Ensure the path to your CSV is correct\n",
    "# Assuming 'goemotions_filtered.csv' already contains data filtered as per the paper\n",
    "df = pd.read_csv('data/goemotions/goemotions_filtered.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Get the one-hot encoded labels\n",
    "# Assuming the last 27 columns are the emotion labels\n",
    "labels_df = df.iloc[:, -27:] # Keep as DataFrame temporarily for easier manipulation\n",
    "labels = labels_df.values # Convert to numpy array\n",
    "\n",
    "# check dimensions\n",
    "print(f\"\\nNumber of examples: {len(texts)}\")\n",
    "print(f\"Example text: {texts[0][:100]}...\")\n",
    "print(f\"Example label shape: {labels[0].shape}\")\n",
    "\n",
    "# Identify and remove rows with emotion labels that appear only once\n",
    "# This is necessary because train_test_split cannot split single instances of a class\n",
    "print(\"\\nChecking for and removing rows with single-instance emotion labels...\")\n",
    "\n",
    "# Calculate the sum for each emotion label across all rows\n",
    "label_counts = labels_df.sum(axis=0)\n",
    "\n",
    "# Identify labels that have a count of 1\n",
    "single_instance_labels = label_counts[label_counts == 1].index.tolist()\n",
    "\n",
    "if single_instance_labels:\n",
    "    print(f\"Found single-instance labels: {single_instance_labels}\")\n",
    "    # Find the indices of rows that have *any* of these single-instance labels\n",
    "    rows_to_remove_indices = df[labels_df[single_instance_labels].sum(axis=1) > 0].index\n",
    "\n",
    "    print(f\"Removing {len(rows_to_remove_indices)} rows containing single-instance labels.\")\n",
    "\n",
    "    # Remove these rows from the dataframe\n",
    "    df_filtered_for_split = df.drop(rows_to_remove_indices)\n",
    "\n",
    "    # Re-extract texts and labels from the filtered dataframe\n",
    "    texts_filtered = df_filtered_for_split['text'].tolist()\n",
    "    labels_filtered = df_filtered_for_split.iloc[:, -27:].values\n",
    "    print(f\"Dataset shape after removing single-instance label rows: {df_filtered_for_split.shape}\")\n",
    "else:\n",
    "    print(\"No single-instance labels found. Proceeding with original data.\")\n",
    "    texts_filtered = texts\n",
    "    labels_filtered = labels\n",
    "\n",
    "# 2. Split data into train, validation, and test sets (80/10/10 split)\n",
    "# Use the filtered data for splitting\n",
    "print(\"\\nSplitting data into train, validation, and test sets...\")\n",
    "# First, split into training (80%) and temp (20% for validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts_filtered, labels_filtered, test_size=0.2, random_state=42 # Removed stratify due to classes with only one member\n",
    ")\n",
    "\n",
    "# Then, split the temp set into validation (10% of total, 50% of temp) and test (10% of total, 50% of temp)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42 # Removed stratify due to classes with only one member\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be0c43e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing and encoding data splits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# BERT and model\n",
    "\n",
    "# 3. Tokenize using BERT tokenizer\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "max_len = 100 # Keep the same max length for consistency, adjust if needed\n",
    "\n",
    "# Tokenize and encode each split separately\n",
    "print(\"\\nTokenizing and encoding data splits...\")\n",
    "\n",
    "encoded_train = tokenizer(\n",
    "    X_train,\n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf' # Return TensorFlow tensors\n",
    ")\n",
    "X_train_input_ids = encoded_train['input_ids']\n",
    "X_train_attention_mask = encoded_train['attention_mask']\n",
    "\n",
    "encoded_val = tokenizer(\n",
    "    X_val,\n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "X_val_input_ids = encoded_val['input_ids']\n",
    "X_val_attention_mask = encoded_val['attention_mask']\n",
    "\n",
    "encoded_test = tokenizer(\n",
    "    X_test,\n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "X_test_input_ids = encoded_test['input_ids']\n",
    "X_test_attention_mask = encoded_test['attention_mask']\n",
    "\n",
    "# Convert labels to TensorFlow tensors if needed, though numpy arrays work with model.fit\n",
    "y_train_tf = tf.constant(y_train, dtype=tf.float32)\n",
    "y_val_tf = tf.constant(y_val, dtype=tf.float32)\n",
    "y_test_tf = tf.constant(y_test, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# 4. Load pre-trained BERT model\n",
    "bert_model = TFBertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# Freeze the BERT model weights during initial training\n",
    "bert_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33535068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 100)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertMod  TFBaseModelOutputWithPooli   1094822   ['input_ids[0][0]',           \n",
      " el)                         ngAndCrossAttentions(last_   40         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, 100, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dropout_113 (Dropout)       (None, 768)                  0         ['tf_bert_model_2[0][1]']     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 64)                   49216     ['dropout_113[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_114 (Dropout)       (None, 64)                   0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " output_layer (Dense)        (None, 27)                   1755      ['dropout_114[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109533211 (417.84 MB)\n",
      "Trainable params: 50971 (199.11 KB)\n",
      "Non-trainable params: 109482240 (417.64 MB)\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Training the model...\n",
      "Epoch 1/4\n",
      "  881/10391 [=>............................] - ETA: 7:50:38 - loss: 0.1751 - accuracy: 0.2306"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_attention_mask\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_attention_mask\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_tf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 7. Evaluate the model on the test set (only done once the model is finalized)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating the model on the test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5. Build the model with BERT and a classification head\n",
    "input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Pass inputs through the BERT model, use pooled output\n",
    "bert_output = bert_model(input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "# Add classification layers on top of BERT output\n",
    "x = layers.Dropout(0.2)(bert_output)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "output_layer = layers.Dense(27, activation='sigmoid', name='output_layer')(x) # 27 emotion labels\n",
    "\n",
    "model = models.Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 6. Train the model using the train and validation sets\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "print(\"\\nTraining the model...\")\n",
    "history = model.fit(\n",
    "    {'input_ids': X_train_input_ids, 'attention_mask': X_train_attention_mask},\n",
    "    y_train_tf,\n",
    "    epochs=4,\n",
    "    batch_size=16,\n",
    "    validation_data=({'input_ids': X_val_input_ids, 'attention_mask': X_val_attention_mask}, y_val_tf),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 7. Evaluate the model on the test set (only done once the model is finalized)\n",
    "print(\"\\nEvaluating the model on the test set...\")\n",
    "loss, accuracy = model.evaluate(\n",
    "    {'input_ids': X_test_input_ids, 'attention_mask': X_test_attention_mask},\n",
    "    y_test_tf,\n",
    "    batch_size=16\n",
    ")\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# 8. Save model\n",
    "#model.save('emotion_model_bert_80_10_10_split.keras')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e979bb5",
   "metadata": {},
   "source": [
    "Let's use a model citated in one paper of this model but using GloVe approach checking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3ca2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Preprocessing function (Paper 1 style)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)            \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)           \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()       \n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/goemotions/goemotions_filtered.csv')\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Token limit filtering (keep between 2 and 30 tokens)\n",
    "df['token_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df = df[df['token_count'].between(2, 30)]\n",
    "df.drop(columns=['token_count'], inplace=True)\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df['text'].tolist()\n",
    "labels = df.iloc[:, -27:].values  # 27 emotion labels\n",
    "\n",
    "# Tokenization\n",
    "max_words = 10000\n",
    "max_len = 30\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "y = np.array(labels)\n",
    "\n",
    "# Train/dev/test split (Paper 1: 80% train, 10% dev, 10% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_dim = 50\n",
    "embedding_index = {}\n",
    "glove_path = 'glove.6B.50d.txt'\n",
    "with open(glove_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Build embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        vec = embedding_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f74d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 30, 50)            500000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 26, 128)           32128     \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 128)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 27)                3483      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 552123 (2.11 MB)\n",
      "Trainable params: 52123 (203.61 KB)\n",
      "Non-trainable params: 500000 (1.91 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "10352/10352 [==============================] - 86s 8ms/step - loss: 0.1842 - accuracy: 0.2246 - val_loss: 0.1522 - val_accuracy: 0.2706\n",
      "Epoch 2/10\n",
      "10352/10352 [==============================] - 81s 8ms/step - loss: 0.1533 - accuracy: 0.2780 - val_loss: 0.1454 - val_accuracy: 0.3024\n",
      "Epoch 3/10\n",
      "10352/10352 [==============================] - 85s 8ms/step - loss: 0.1465 - accuracy: 0.3035 - val_loss: 0.1405 - val_accuracy: 0.3196\n",
      "Epoch 4/10\n",
      "10352/10352 [==============================] - 79s 8ms/step - loss: 0.1421 - accuracy: 0.3155 - val_loss: 0.1375 - val_accuracy: 0.3262\n",
      "Epoch 5/10\n",
      "10352/10352 [==============================] - 78s 7ms/step - loss: 0.1391 - accuracy: 0.3225 - val_loss: 0.1353 - val_accuracy: 0.3320\n",
      "Epoch 6/10\n",
      "10352/10352 [==============================] - 78s 8ms/step - loss: 0.1371 - accuracy: 0.3273 - val_loss: 0.1339 - val_accuracy: 0.3357\n",
      "Epoch 7/10\n",
      "10352/10352 [==============================] - 78s 8ms/step - loss: 0.1355 - accuracy: 0.3321 - val_loss: 0.1327 - val_accuracy: 0.3385\n",
      "Epoch 8/10\n",
      "10352/10352 [==============================] - 77s 7ms/step - loss: 0.1343 - accuracy: 0.3348 - val_loss: 0.1318 - val_accuracy: 0.3414\n",
      "Epoch 9/10\n",
      "10352/10352 [==============================] - 77s 7ms/step - loss: 0.1334 - accuracy: 0.3378 - val_loss: 0.1310 - val_accuracy: 0.3457\n",
      "Epoch 10/10\n",
      "10352/10352 [==============================] - 78s 8ms/step - loss: 0.1324 - accuracy: 0.3413 - val_loss: 0.1304 - val_accuracy: 0.3471\n",
      "647/647 [==============================] - 3s 5ms/step - loss: 0.1309 - accuracy: 0.3462\n",
      "\n",
      "Test Loss: 0.1309, Test Accuracy: 0.3462\n"
     ]
    }
   ],
   "source": [
    "# CNN model from Paper 1 (non-BERT baseline)\n",
    "model_paper_1 = models.Sequential([\n",
    "    layers.Input(shape=(max_len,)),\n",
    "    layers.Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=False\n",
    "    ),\n",
    "    layers.Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(27, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_paper_1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_paper_1.summary()\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model_paper_1.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=16,  # Per Paper 1\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_paper_1.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Save\n",
    "#model_paper_1.save('glove_cnn_emotion_model_paper_1.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3265e3",
   "metadata": {},
   "source": [
    "Let's also check other measures for this as F1 Score, Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc4ef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "647/647 [==============================] - 3s 4ms/step\n",
      "[[0.01282421 0.0060567  0.02719866 ... 0.07010121 0.02137885 0.19038928]\n",
      " [0.33976814 0.06680038 0.12600544 ... 0.00922946 0.01667458 0.16431417]\n",
      " [0.0059865  0.00387016 0.01877972 ... 0.02847379 0.01070489 0.22515517]\n",
      " ...\n",
      " [0.02139626 0.06044342 0.0661814  ... 0.01927025 0.0389699  0.45850116]\n",
      " [0.04290602 0.03060021 0.07629908 ... 0.00409844 0.01940195 0.35640788]\n",
      " [0.01302919 0.01293397 0.01934005 ... 0.01312545 0.0076209  0.08929104]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "# make the predictions\n",
    "y_pred_probs = model_paper_1.predict(X_test)\n",
    "print(y_pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fcbb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a threshold to classifiy one emotion in the end \n",
    "# we can tune this threshold later by looking at the different results in metrics it gives different thresholds\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "# metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986c53d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1559246785058175\n",
      "0.07034166726858461\n"
     ]
    }
   ],
   "source": [
    "print(f1_micro)\n",
    "print(f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f15df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this threshold:  0.3\n",
      "0.3092157613981315\n",
      "0.11816237404617404\n",
      "For this threshold:  0.4\n",
      "0.2311104949396922\n",
      "0.08917674478729902\n",
      "For this threshold:  0.5\n",
      "0.1559246785058175\n",
      "0.07034166726858461\n",
      "For this threshold:  0.6\n",
      "0.10355387218953985\n",
      "0.05592704056700485\n",
      "For this threshold:  0.7\n",
      "0.06371225651036258\n",
      "0.039453039046557455\n"
     ]
    }
   ],
   "source": [
    "# play with different thresholds for the above model\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for threshold in thresholds:\n",
    "    print(\"For this threshold: \", threshold)\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    print(f1_micro)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f1_macro)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a8231",
   "metadata": {},
   "source": [
    "Let's take into account paper 1: \n",
    "The pre-processing papers:\n",
    "- Text cleaning (lowercasing, removing punctuation and URLs, whitespace normalization).\n",
    "- Token count filtering (keep comments between 2 and 30 tokens).\n",
    "- Tokenization and padding.\n",
    "- Class imbalance correction using per-label weighting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d215fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "#1. Preprocess text \n",
    "def clean_text(text):\n",
    "    text = text.lower()                                 # Lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)                # Remove URLs\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)               # Remove punctuation/special chars\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()           # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "#2. Load and prepare dataset ---\n",
    "df = pd.read_csv('data/goemotions/goemotions_filtered.csv')\n",
    "\n",
    "# Clean text\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df['token_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df = df[df['token_count'].between(2, 30)]              \n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df['text'].tolist()\n",
    "labels = df.iloc[:, -27:].values                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7927717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "max_words = 10000\n",
    "max_len = 30\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split into train/val/test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#Compute class weights to handle imbalance ---\n",
    "label_frequencies = np.sum(y_train, axis=0)\n",
    "label_weights = 1.0 / (label_frequencies + 1e-6)\n",
    "label_weights = label_weights / np.sum(label_weights) * len(label_weights)\n",
    "label_weights_tensor = tf.convert_to_tensor(label_weights, dtype=tf.float32)\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_mean(bce * label_weights_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30767b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Load GloVe embeddings\n",
    "embedding_dim = 50\n",
    "embedding_index = {}\n",
    "with open('glove.6B.50d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        vec = embedding_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "923f4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 30, 50)            500000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 512)               628736    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 27)                3483      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1197883 (4.57 MB)\n",
      "Trainable params: 697883 (2.66 MB)\n",
      "Non-trainable params: 500000 (1.91 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build an LSTM model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model_LSTM = models.Sequential([\n",
    "    layers.Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
    "    layers.Bidirectional(layers.LSTM(256)), # Hidden layer dimensionality is 256\n",
    "    layers.Dropout(0.7),                    # Dropout set to 0.7\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(27, activation='sigmoid')\n",
    "])\n",
    "\n",
    "custom_optimizer = Adam(learning_rate=0.1)\n",
    "\n",
    "model_LSTM.compile(\n",
    "    optimizer=custom_optimizer, # Use the custom optimizer\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83797d20",
   "metadata": {},
   "source": [
    "I stopped in my computer because it was taking too long to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7213c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#5. Train and evaluate ---\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel_LSTM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m model_LSTM\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom LSTM Model Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#5. Train and evaluate ---\n",
    "model_LSTM.fit(X_train, y_train, epochs=4, batch_size=16, validation_data=(X_val, y_val), verbose=2)\n",
    "loss, acc = model_LSTM.evaluate(X_test, y_test)\n",
    "print(f\"Custom LSTM Model Test Loss: {loss:.4f}, Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b2a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check other metrics such as f1 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the same data without the pre-processing steps used above (See if the results are different)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451a588",
   "metadata": {},
   "source": [
    "According to other paper let's use the initial goemotions data and apply the following pre-processing steps used in the paper \"Fine-Grained Classification for Emotion Detection Using Advanced Neural Models and GoEmotions Dataset\" - 3rd paper\n",
    "\n",
    "The pre-processing steps used in this paper are:\n",
    "- Emoji Conversion\n",
    "- Contraction Expansion\n",
    "- Acronym and Misspelling Correction\n",
    "- Text Normalization\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1f7ff",
   "metadata": {},
   "source": [
    "Let's use the architecture as well as the parameters used which are: \n",
    "Paper's CNN: Embedding -> Conv1D(256 filters) -> Dropout -> Dense(output)\n",
    "Learning rate: 0.0002, Epochs: 12, Optimizer: Adam, Loss: Binary Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd644e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (207814, 37)\n",
      "\n",
      "First few rows of the dataset:\n",
      "                                                text       id  \\\n",
      "0                                    That game hurt.  eew5j0j   \n",
      "1     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
      "2                                 Man I love reddit.  eeibobj   \n",
      "3  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
      "4  Right? Considering it’s such an important docu...  eespn2i   \n",
      "\n",
      "                author            subreddit    link_id   parent_id  \\\n",
      "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
      "1             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
      "2        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
      "3  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
      "4         ImperialBoss           TrueReddit  t3_aizyuz  t1_eesoak0   \n",
      "\n",
      "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
      "0  1.548381e+09         1                 False           0  ...     0   \n",
      "1  1.546428e+09        37                 False           0  ...     0   \n",
      "2  1.547965e+09        18                 False           0  ...     1   \n",
      "3  1.546669e+09         2                 False           0  ...     0   \n",
      "4  1.548280e+09        61                 False           0  ...     0   \n",
      "\n",
      "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0            0         0      0            0       0        0        1   \n",
      "1            0         0      0            0       0        0        0   \n",
      "2            0         0      0            0       0        0        0   \n",
      "3            0         0      0            0       0        0        0   \n",
      "4            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        1  \n",
      "2         0        0  \n",
      "3         0        1  \n",
      "4         0        0  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "\n",
      "Original Column names:\n",
      "['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "\n",
      "Identified label columns: ['amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "\n",
      "Applying pre-processing to texts...\n",
      "Shape of X_padded (all data before split): (207814, 100)\n",
      "Shape of y_labels (all labels before split): (207814, 27), dtype: float32\n",
      "Shape of X_train_val (training + validation data): (166251, 100)\n",
      "Shape of y_train_val (training + validation labels): (166251, 27), dtype: float32\n",
      "Shape of X_test (test data): (41563, 100)\n",
      "Shape of y_test (test labels): (41563, 27), dtype: float32\n",
      "Found 400000 word vectors in GloVe.\n",
      "Shape of embedding matrix: (10000, 50)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 100, 50)           500000    \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 98, 256)           38656     \n",
      "                                                                 \n",
      " global_max_pooling1d_5 (Gl  (None, 256)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 27)                6939      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 545595 (2.08 MB)\n",
      "Trainable params: 45595 (178.11 KB)\n",
      "Non-trainable params: 500000 (1.91 MB)\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting model training...\n",
      "Epoch 1/12\n",
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\UCP_APA_2425\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3897/3897 [==============================] - 55s 14ms/step - loss: 0.1563 - f1_score: 0.0689 - accuracy: 0.2893 - val_loss: 0.1387 - val_f1_score: 0.1223 - val_accuracy: 0.3304\n",
      "Epoch 2/12\n",
      "3897/3897 [==============================] - 69s 18ms/step - loss: 0.1372 - f1_score: 0.1507 - accuracy: 0.3323 - val_loss: 0.1329 - val_f1_score: 0.1759 - val_accuracy: 0.3418\n",
      "Epoch 3/12\n",
      "3897/3897 [==============================] - 73s 19ms/step - loss: 0.1331 - f1_score: 0.1687 - accuracy: 0.3431 - val_loss: 0.1306 - val_f1_score: 0.1664 - val_accuracy: 0.3495\n",
      "Epoch 4/12\n",
      "3897/3897 [==============================] - 69s 18ms/step - loss: 0.1309 - f1_score: 0.1781 - accuracy: 0.3493 - val_loss: 0.1291 - val_f1_score: 0.1626 - val_accuracy: 0.3582\n",
      "Epoch 5/12\n",
      "3897/3897 [==============================] - 60s 15ms/step - loss: 0.1293 - f1_score: 0.1857 - accuracy: 0.3546 - val_loss: 0.1285 - val_f1_score: 0.1840 - val_accuracy: 0.3613\n",
      "Epoch 6/12\n",
      "3897/3897 [==============================] - 60s 15ms/step - loss: 0.1282 - f1_score: 0.1920 - accuracy: 0.3583 - val_loss: 0.1273 - val_f1_score: 0.2059 - val_accuracy: 0.3618\n",
      "Epoch 7/12\n",
      "3897/3897 [==============================] - 65s 17ms/step - loss: 0.1273 - f1_score: 0.1998 - accuracy: 0.3615 - val_loss: 0.1266 - val_f1_score: 0.1849 - val_accuracy: 0.3657\n",
      "Epoch 8/12\n",
      "3897/3897 [==============================] - 59s 15ms/step - loss: 0.1267 - f1_score: 0.2024 - accuracy: 0.3631 - val_loss: 0.1261 - val_f1_score: 0.1864 - val_accuracy: 0.3679\n",
      "Epoch 9/12\n",
      "3895/3897 [============================>.] - ETA: 0s - loss: 0.1260 - f1_score: 0.2076 - accuracy: 0.3667Restoring model weights from the end of the best epoch: 6.\n",
      "3897/3897 [==============================] - 67s 17ms/step - loss: 0.1260 - f1_score: 0.2076 - accuracy: 0.3667 - val_loss: 0.1257 - val_f1_score: 0.1835 - val_accuracy: 0.3692\n",
      "Epoch 9: early stopping\n",
      "\n",
      "Evaluating model on the test set...\n",
      "Test Set Evaluation:\n",
      "loss: 0.1263\n",
      "f1_score: 0.2069\n",
      "accuracy: 0.3644\n",
      "\n",
      "Model saved to emotion_model_cnn_paper_aligned.keras\n",
      "1/1 [==============================] - 0s 229ms/step\n",
      "\n",
      "Example prediction for: I am so incredibly happy and excited today, it feels amazing! 😄🎉\n",
      "joy: 0.1576\n",
      "excitement: 0.1206\n",
      "approval: 0.0514\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "\n",
      "Example prediction for: This is so frustrating and annoying, I can't believe this happened... 😠\n",
      "annoyance: 0.2130\n",
      "disapproval: 0.1657\n",
      "fear: 0.1482\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam # Import Adam optimizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import F1Score # Import F1Score\n",
    "import re\n",
    "import emoji # For emoji conversion\n",
    "import contractions # For expanding contractions\n",
    "from sklearn.model_selection import train_test_split # Import for train-test split\n",
    "\n",
    "# Hides the GPU from TensorFlow if not needed or causing issues\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# --- Pre-processing Function based on the Paper ---\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Applies pre-processing steps as described in the paper:\n",
    "    1. Convert Emojis to text\n",
    "    2. Expand Contractions\n",
    "    3. Fix specific Acronyms and Misspellings\n",
    "    4. Lowercase text\n",
    "    5. Normalize repeated characters\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\" # Return empty string for non-string inputs\n",
    "\n",
    "    # 1. Convert Emojis to text\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \")) # e.g., 👍 -> thumbs_up\n",
    "\n",
    "    # 2. Expand Contractions\n",
    "    text = contractions.fix(text) # e.g., \"I'll\" -> \"I will\"\n",
    "\n",
    "    # 3. Fix specific Acronyms and Misspellings (examples from paper)\n",
    "    text = re.sub(r'\\b(Cuz|coz)\\b', 'because', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(Ikr)\\b', 'I know right', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(Faux pas)\\b', 'mistake', text, flags=re.IGNORECASE)\n",
    "    # Add more rules as needed\n",
    "\n",
    "    # 4. Lowercase text\n",
    "    text = text.lower()\n",
    "\n",
    "    # 5. Normalize repeated characters (e.g., \"coooool\" -> \"cool\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text) # Reduces 3 or more repetitions to 2\n",
    "                                            # For \"cool\", change to r'\\1' if only one char is desired\n",
    "\n",
    "    # Remove extra spaces that might have been introduced\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "# Ensure the path to your CSV is correct\n",
    "try:\n",
    "    df = pd.read_csv('data/goemotions/goemotions_filtered.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'data/goemotions/goemotions_filtered.csv' not found.\")\n",
    "    print(\"Please ensure the dataset is in the correct path or update the path in the script.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())\n",
    "original_columns = df.columns.tolist() # Get column names BEFORE any modifications\n",
    "print(\"\\nOriginal Column names:\")\n",
    "print(original_columns)\n",
    "\n",
    "# Get the one-hot encoded labels from the original DataFrame\n",
    "# The paper mentions 28 emotions. This script assumes 27 based on previous context.\n",
    "# It's assumed the label columns are the last N columns in the *original* CSV.\n",
    "num_label_columns = 27 # Adjust if necessary based on your CSV structure\n",
    "if len(original_columns) < num_label_columns:\n",
    "    print(f\"Error: DataFrame has fewer than {num_label_columns} columns. Cannot extract labels as expected.\")\n",
    "    exit()\n",
    "\n",
    "label_column_names = original_columns[-num_label_columns:]\n",
    "print(f\"\\nIdentified label columns: {label_column_names}\")\n",
    "\n",
    "# Extract labels using the identified column names from the original DataFrame\n",
    "try:\n",
    "    labels_from_df = df[label_column_names].values\n",
    "    # Verify that all label columns are numeric\n",
    "    if not np.issubdtype(labels_from_df.dtype, np.number):\n",
    "        print(f\"Warning: Labels extracted from columns {label_column_names} are not all numeric. Attempting conversion.\")\n",
    "        # This conversion might fail if there's genuine non-numeric text\n",
    "        labels_from_df = df[label_column_names].astype(float).values\n",
    "except KeyError as e:\n",
    "    print(f\"Error extracting label columns: {e}. Check column names in your CSV and `num_label_columns`.\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"Error converting label columns to numeric: {e}. One of the identified label columns likely contains non-numeric text.\")\n",
    "    exit()\n",
    "\n",
    "# Extract texts and apply pre-processing\n",
    "print(\"\\nApplying pre-processing to texts...\")\n",
    "# Ensure 'text' column exists\n",
    "if 'text' not in df.columns:\n",
    "    print(\"Error: 'text' column not found in DataFrame. Please check your CSV file.\")\n",
    "    exit()\n",
    "df['processed_text'] = df['text'].apply(preprocess_text) # Now add the processed_text column\n",
    "texts = df['processed_text'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# 2. Tokenize and pad\n",
    "max_words = 10000  # Max words to keep in the vocabulary\n",
    "max_len = 100      # Max length of sequences\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<unk>\") # Added oov_token\n",
    "tokenizer.fit_on_texts(texts) # Fit tokenizer on ALL texts to build comprehensive vocabulary\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X_padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post') # Padded sequences\n",
    "\n",
    "# Ensure labels are of type float32 for TensorFlow/Keras\n",
    "y_labels = np.array(labels_from_df, dtype=np.float32)\n",
    "\n",
    "\n",
    "print(f\"Shape of X_padded (all data before split): {X_padded.shape}\")\n",
    "print(f\"Shape of y_labels (all labels before split): {y_labels.shape}, dtype: {y_labels.dtype}\")\n",
    "\n",
    "# 2.b. Split data into Training, Validation, and Test sets\n",
    "try:\n",
    "    stratify_target = None\n",
    "    if y_labels.ndim > 1 and y_labels.shape[1] > 1: # Multi-label one-hot\n",
    "        stratify_target = np.argmax(y_labels, axis=1) # Simplification for stratification\n",
    "    elif y_labels.ndim == 1: # Single-label\n",
    "        stratify_target = y_labels\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X_padded, y_labels, test_size=0.2, random_state=42, stratify=stratify_target\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Stratification failed: {e}. Falling back to non-stratified split.\")\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X_padded, y_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Shape of X_train_val (training + validation data): {X_train_val.shape}\")\n",
    "print(f\"Shape of y_train_val (training + validation labels): {y_train_val.shape}, dtype: {y_train_val.dtype}\")\n",
    "print(f\"Shape of X_test (test data): {X_test.shape}\")\n",
    "print(f\"Shape of y_test (test labels): {y_test.shape}, dtype: {y_test.dtype}\")\n",
    "\n",
    "\n",
    "# 3. Load GloVe embeddings\n",
    "embedding_dim = 50  # GloVe embeddings dimension (e.g., 50, 100, 200, 300)\n",
    "embeddings_index = {}\n",
    "# Ensure the path to your GloVe file is correct\n",
    "glove_path = 'glove.6B.50d.txt' # Using 50d embeddings\n",
    "try:\n",
    "    with open(glove_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Found {len(embeddings_index)} word vectors in GloVe.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: GloVe file '{glove_path}' not found.\")\n",
    "    print(\"Please download GloVe embeddings (e.g., glove.6B.50d.txt) and place it in the correct path or update the path.\")\n",
    "    exit()\n",
    "\n",
    "# 4. Prepare embedding matrix\n",
    "word_index = tokenizer.word_index # Use the same tokenizer fitted on all texts\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words: # Use num_words which is min(max_words, actual_vocab_size+1)\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(f\"Shape of embedding matrix: {embedding_matrix.shape}\")\n",
    "\n",
    "# 5. Build the CNN model (aligned with the paper)\n",
    "# Paper's CNN: Embedding -> Conv1D(256 filters) -> Dropout -> Dense(output)\n",
    "# Learning rate: 0.0002, Epochs: 12, Optimizer: Adam, Loss: Binary Cross-entropy\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_len, \n",
    "        trainable=False  \n",
    "    ),\n",
    "    layers.Conv1D(\n",
    "        filters=256,      \n",
    "        kernel_size=3,    \n",
    "        activation='relu' \n",
    "    ),\n",
    "    layers.GlobalMaxPooling1D(), \n",
    "    layers.Dropout(0.2),                                   \n",
    "    layers.Dense(y_labels.shape[1], activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Optimizer with learning rate from the paper\n",
    "custom_optimizer = Adam(learning_rate=0.0002)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=custom_optimizer,\n",
    "    loss='binary_crossentropy', # As per paper\n",
    "    metrics=[F1Score(average='micro', threshold=0.5, name='f1_score'), 'accuracy'] # Use F1Score class, 'micro' is good for multi-label\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 6. Train the model\n",
    "# Paper specifies 12 epochs for the CNN model\n",
    "epochs_from_paper = 12\n",
    "batch_size = 32 # Common batch size, paper doesn't specify for CNN\n",
    "\n",
    "# Early stopping is a good practice, though not explicitly mentioned for the CNN in the paper\n",
    "early_stop = EarlyStopping(monitor='val_f1_score', mode='max', patience=3, restore_best_weights=True, verbose=1) # Monitor val_f1_score\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(\n",
    "    X_train_val, y_train_val, # Train on the training+validation split\n",
    "    epochs=epochs_from_paper,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.25, # Create validation set from X_train_val (0.25 of 0.8 original data = 0.2 of total)\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 7. Evaluate on Test Set (Important for unbiased performance measure)\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "test_metric_names = model.metrics_names\n",
    "print(\"Test Set Evaluation:\")\n",
    "for name, value in zip(test_metric_names, test_results):\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "\n",
    "# 8. Save model\n",
    "model_save_path = 'emotion_model_cnn_paper_aligned.keras'\n",
    "model.save(model_save_path)\n",
    "print(f\"\\nModel saved to {model_save_path}\")\n",
    "\n",
    "# 9. Make predictions function\n",
    "def predict_emotion(text_input, trained_model, tokenizer_instance, max_len_sequences, current_label_column_names):\n",
    "    \"\"\"Predicts top 3 emotions for a given text.\"\"\"\n",
    "    # Pre-process the input text\n",
    "    processed_text_input = preprocess_text(text_input)\n",
    "    \n",
    "    # Tokenize and pad the input text\n",
    "    sequence = tokenizer_instance.texts_to_sequences([processed_text_input])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len_sequences, padding='post', truncating='post')\n",
    "    \n",
    "    # Get prediction\n",
    "    if padded_sequence.shape[0] == 0: # Handle empty sequence after tokenization\n",
    "        print(\"Warning: Text could not be tokenized effectively.\")\n",
    "        return []\n",
    "        \n",
    "    prediction_probs = trained_model.predict(padded_sequence)[0]\n",
    "    \n",
    "    # Get the top 3 emotions\n",
    "    top_3_indices = prediction_probs.argsort()[-3:][::-1] # Indices of top 3 scores\n",
    "    \n",
    "    top_3_emotions_with_scores = []\n",
    "    for idx in top_3_indices:\n",
    "        if idx < len(current_label_column_names):\n",
    "             top_3_emotions_with_scores.append((current_label_column_names[idx], prediction_probs[idx]))\n",
    "        else:\n",
    "            print(f\"Warning: Predicted index {idx} is out of bounds for emotion labels.\")\n",
    "\n",
    "    return top_3_emotions_with_scores\n",
    "\n",
    "# Example prediction\n",
    "test_text = \"I am so incredibly happy and excited today, it feels amazing! 😄🎉\"\n",
    "# Use the 'label_column_names' identified during data loading\n",
    "predictions = predict_emotion(test_text, model, tokenizer, max_len, label_column_names)\n",
    "print(\"\\nExample prediction for:\", test_text)\n",
    "if predictions:\n",
    "    for emotion_label, score in predictions:\n",
    "        print(f\"{emotion_label}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"No predictions could be made.\")\n",
    "\n",
    "test_text_2 = \"This is so frustrating and annoying, I can't believe this happened... 😠\"\n",
    "predictions_2 = predict_emotion(test_text_2, model, tokenizer, max_len, label_column_names)\n",
    "print(\"\\nExample prediction for:\", test_text_2)\n",
    "if predictions_2:\n",
    "    for emotion_label, score in predictions_2:\n",
    "        print(f\"{emotion_label}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"No predictions could be made.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1573c08c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UCP_APA_2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
