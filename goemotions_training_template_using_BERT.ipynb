{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04659649",
   "metadata": {},
   "source": [
    "Let's in this case instead of using GloVe let's use having the same structure as before the BERT tranformer, using its embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48292074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (207814, 37)\n",
      "\n",
      "First few rows of the dataset:\n",
      "                                                text       id  \\\n",
      "0                                    That game hurt.  eew5j0j   \n",
      "1     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
      "2                                 Man I love reddit.  eeibobj   \n",
      "3  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
      "4  Right? Considering itâ€™s such an important docu...  eespn2i   \n",
      "\n",
      "                author            subreddit    link_id   parent_id  \\\n",
      "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
      "1             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
      "2        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
      "3  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
      "4         ImperialBoss           TrueReddit  t3_aizyuz  t1_eesoak0   \n",
      "\n",
      "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
      "0  1.548381e+09         1                 False           0  ...     0   \n",
      "1  1.546428e+09        37                 False           0  ...     0   \n",
      "2  1.547965e+09        18                 False           0  ...     1   \n",
      "3  1.546669e+09         2                 False           0  ...     0   \n",
      "4  1.548280e+09        61                 False           0  ...     0   \n",
      "\n",
      "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0            0         0      0            0       0        0        1   \n",
      "1            0         0      0            0       0        0        0   \n",
      "2            0         0      0            0       0        0        0   \n",
      "3            0         0      0            0       0        0        0   \n",
      "4            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        1  \n",
      "2         0        0  \n",
      "3         0        1  \n",
      "4         0        0  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "\n",
      "Column names:\n",
      "['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "\n",
      "Number of training examples: 207814\n",
      "Example text: That game hurt....\n",
      "Example label shape: (27,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     46\u001b[0m embeddings_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 47\u001b[0m current_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[0;32m     48\u001b[0m glove_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove.6B.50d.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(glove_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Hides the GPU from TensorFlow\n",
    "tf.config.set_visible_devices([], 'GPU') \n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('data/goemotions/goemotions_filtered.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Get the one-hot encoded labels\n",
    "labels = df.iloc[:, -27:].values  # Convert to numpy array\n",
    "\n",
    "# check dimensions\n",
    "print(f\"\\nNumber of training examples: {len(texts)}\")\n",
    "print(f\"Example text: {texts[0][:100]}...\")\n",
    "print(f\"Example label shape: {labels[0].shape}\")\n",
    "\n",
    "# 2. Tokenize and pad\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "y = np.array(labels)\n",
    "\n",
    "# 3. Load GloVe embeddings\n",
    "embedding_dim = 50\n",
    "embeddings_index = {}\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "glove_path = 'glove.6B.50d.txt'\n",
    "with open(glove_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    " \n",
    "# 4. Prepare embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c43e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UCP_APA_2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
