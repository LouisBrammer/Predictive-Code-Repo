{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Hides the GPU from TensorFlow\n",
    "tf.config.set_visible_devices([], 'GPU') \n",
    "\n",
    "# Base path for the dataset\n",
    "dataset_path = 'data/aclImdb'\n",
    "\n",
    "train_dataset = keras.utils.text_dataset_from_directory(os.path.expanduser(dataset_path), batch_size=32)    #batch size needs to be changed here\n",
    "valid_dataset = keras.utils.text_dataset_from_directory(os.path.expanduser(dataset_path), batch_size=32)    #batch size needs to be changed here\n",
    "\n",
    "\n",
    "# 1. Prepare text data from dataset\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for text_batch, label_batch in train_dataset:\n",
    "    for text, label in zip(text_batch.numpy(), label_batch.numpy()):\n",
    "        texts.append(text.decode('utf-8'))\n",
    "        labels.append([label]) # Convert to list for consistency\n",
    "\n",
    "print(f\"Number of training examples: {len(texts)}\")\n",
    "print(f\"Example text: {texts[0][:100]}...\")\n",
    "print(f\"Example label: {labels[0]}\")\n",
    "\n",
    "# 2. Tokenize and pad\n",
    "\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "y = np.array(labels)\n",
    "\n",
    "# 3. Load GloVe embeddings\n",
    "embedding_dim = 50\n",
    "embeddings_index = {}\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "glove_path = 'glove.6B.50d.txt'\n",
    "with open(glove_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    " \n",
    "# 4. Prepare embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHANGE FROM HERE ONWARDS\n",
    "\n",
    "# 5. Build a simple model\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(max_len,)),\n",
    "    layers.Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False\n",
    "    ),\n",
    "    layers.Conv1D(64, 3, activation='relu'),  # Add local feature extraction\n",
    "    layers.GlobalMaxPooling1D(),  # Max pooling captures the most important features\n",
    "    layers.Dropout(0.2),  # Add regularization to prevent overfitting\n",
    "    layers.Dense(32, activation='relu'),  # Increase from 16 to 32\n",
    "    layers.Dense(16, activation='relu'),  # Add another layer\n",
    "    layers.Dropout(0.2),  # Additional dropout\n",
    "    layers.Dense(y.shape[1], activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']  # Added AUC metric for better evaluation\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# 6. Train\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model.fit(X, y, epochs=10, verbose=1, validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "\n",
    "# 6.B STORE MODEL\n",
    "model.save('model.keras')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
